{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_easily = pd.read_csv(\"C:/Users/benysar/Desktop/easily.csv\", delimiter='|', encoding=\"utf-8\") #CHANGER PATH\n",
    "df_warehouse = pd.read_csv(\"C:/Users/benysar/Desktop/warehouse.csv\", delimiter='|', encoding=\"utf-8\") #CHANGER PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression des doublons parfaits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_easily_nodups = df_easily.drop_duplicates()\n",
    "df_warehouse_nodups = df_easily.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing des dates et des nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des nan en float\n",
    "df_easily_nodups['pat_ipp'] = df_easily_nodups['pat_ipp'].astype(float)\n",
    "df_warehouse_nodups['pat_ipp'] = df_warehouse_nodups['pat_ipp'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les dates au format 'YYYY-MM-DD HH:MM:SS', couper la chaîne aux 10 premiers caractères \n",
    "df_warehouse_nodups['DOCUMENT_DATE'] = df_warehouse_nodups['DOCUMENT_DATE'].str.slice(0, 10)\n",
    "df_easily_nodups['DOCUMENT_DATE'] = df_easily_nodups['DOCUMENT_DATE'].str.slice(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la date, et mergre pour obtenir les différences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1: Créer une nouvelle colonne pour la date du document\n",
    "df_easily_nodups['DOCUMENT_DATE_EASILY'] = df_easily_nodups.apply(\n",
    "    lambda row: row['rea'] if pd.notnull(row['rea']) else (row['crea'] if pd.notnull(row['crea']) else row['modif']), axis=1)\n",
    "\n",
    "df_warehouse_nodups['DOCUMENT_DATE_WAREHOUSE'] = df_warehouse_nodups['DOCUMENT_DATE']\n",
    "\n",
    "# Étape 2: Fusionner les DataFrames sur les ID patients et les noms des documents\n",
    "merged_df = pd.merge(df_easily_nodups, df_warehouse_nodups, left_on=['pat_ipp', 'doc_nom', 'DOCUMENT_DATE_EASILY'], \n",
    "                     right_on=['HOSPITAL_PATIENT_ID', 'TITLE', 'DOCUMENT_DATE_WAREHOUSE'], how='outer', indicator=True)\n",
    "\n",
    "# Étape 3: Filtrer les résultats pour obtenir les différences\n",
    "differences = merged_df[merged_df['_merge'] != 'both']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la répartition des valeurs de fusion\n",
    "merge_counts = merged_df['_merge'].value_counts()\n",
    "\n",
    "# Renommer les valeurs\n",
    "merge_counts = merge_counts.rename(index={\n",
    "    'left_only': 'Présent seulement dans df_easily',\n",
    "    'right_only': 'Présent seulement dans df_warehouse',\n",
    "    'both': 'Présent dans les deux DataFrames'\n",
    "})\n",
    "\n",
    "# Afficher les résultats\n",
    "print(merge_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['DOCUMENT_DATE_EASILY_year'] = merged_df['DOCUMENT_DATE_EASILY'].str.slice(0, 3)\n",
    "\n",
    "merged_df_2023 = merged_df[merged_df['DOCUMENT_DATE_EASILY_year'] == '2023']\n",
    "merge_counts_2023 = merged_df_2023['_merge'].value_counts()\n",
    "\n",
    "# Renommer les valeurs\n",
    "merge_counts_2023 = merge_counts_2023.rename(index={\n",
    "    'left_only': 'Présent seulement dans df_easily en 2023',\n",
    "    'right_only': 'Présent seulement dans df_warehouse en 2023',\n",
    "    'both': 'Présent dans les deux DataFrames en 2023'\n",
    "})\n",
    "\n",
    "# Afficher les résultats\n",
    "print(merge_counts_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détail des documents manquants pour chaque connecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge\n",
    "df = pd.merge(df_easily_nodups, df_warehouse_nodups, \n",
    "                     left_on=['pat_ipp', 'doc_nom', 'DOCUMENT_DATE_EASILY'], \n",
    "                     right_on=['HOSPITAL_PATIENT_ID', 'TITLE', 'DOCUMENT_DATE_WAREHOUSE'], \n",
    "                     how='outer', indicator=True)\n",
    "\n",
    "df = df[df['_merge'] != 'both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter rows where 'DOCUMENT_DATE_EASILY_year' is '2023'\n",
    "df['DOCUMENT_DATE_EASILY_year'] = df['DOCUMENT_DATE_EASILY'].str.slice(0, 3)\n",
    "filtered_df = df[df['DOCUMENT_DATE_EASILY_year'] == '2023']\n",
    "\n",
    "def save_app_nom_data_to_excel(df):\n",
    "    # Base directory where files will be saved\n",
    "    base_dir = \"C:/Users/benysar/testing_data/doc_manquants_warehouse\"  #CHANGER PATH\n",
    "\n",
    "    # Iterate through unique values in 'app_nom'\n",
    "    for app_name in df['app_nom'].unique():\n",
    "        # Sanitize the app_name to remove or replace invalid characters for file names\n",
    "        sanitized_app_name = app_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\").lower()\n",
    "\n",
    "        # Filter the DataFrame for the current app_name\n",
    "        filtered_df = df[df['app_nom'] == app_name]\n",
    "\n",
    "        filtered_df['pat_ipp'] = filtered_df['pat_ipp'].astype(int)\n",
    "\n",
    "        filtered_df = filtered_df[['doc_nom', 'pat_ipp', 'app_nom', 'DOCUMENT_DATE_EASILY']]\n",
    "\n",
    "        # File name based on sanitized app_name\n",
    "        file_name = f\"{base_dir}/{sanitized_app_name}_2023.xlsx\"\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        filtered_df.to_excel(file_name, index=False)\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "\n",
    "# Example usage\n",
    "save_app_nom_data_to_excel(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts and normalized counts\n",
    "counts = filtered_df[\"app_nom\"].value_counts()\n",
    "normalized_counts = filtered_df[\"app_nom\"].value_counts(normalize=True)\n",
    "\n",
    "# Create a new DataFrame with these values\n",
    "df_counts = pd.DataFrame({\n",
    "    'Counts': counts,\n",
    "    'Normalized Counts': normalized_counts\n",
    "})\n",
    "\n",
    "print(df_counts)\n",
    "\n",
    "# Correcting the file path with the proper extension\n",
    "file_path = \"C:/Users/benysar/testing_data/doc_manquants_warehouse/RESUME_MANQUANTS.xlsx\" #CHANGER PATH\n",
    "\n",
    "# Saving the DataFrame to an Excel file\n",
    "df_counts.to_excel(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
